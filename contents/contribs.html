<!-- SETTITLE Other Contributions -->

<h4>Reviewing</h4>

<ul class="spaced">
<li>(occasional journal papers)</li>
<li><b>2024:</b> ICML, ARR </li>
<li><b>2023:</b> ICGI </li>
<li><b>2022:</b> ICLR, NeurIPS, MemARI, EACL (for 2023) </li>
<li><b>2021:</b> ICML, NeurIPS, ACL, EMNLP, ICGI, PLDI
	<ul><b>ACL 2021:</b> <a href="https://2021.aclweb.org/blog/reviewer-list/">Outstanding Reviewer</a></ul>
</li>
<li><b>2020:</b> ICML, NeurIPS
	<ul><b>ICML 2020:</b> Top 33% reviewer</ul>
</li>
</ul>


 <h4>Community</h4>

<img class="floats" src="files/flann.png" alt="FLaNN icon", height="60" width="60">
<div>
<b>2022:</b> I am credited as the founder of the FLaNN (Formal Languages and Neural Networks) discord and seminar in our  <a href="http://flann.super.site/">website</a>, and indeed did invite members and run the seminar for several months in its beginnings (late 2021/2022)! I am since grateful to <a href="http://lambdaviking.com/">Will Merrill</a>, <a href="https://sleynas.com/">Lena Strobl</a>, and <a href="https://mcognetta.github.io">Marco Cognetta</a> for taking over a considerable amount of the work, including creating our website!
</div>

<br class="clearsfloats">

<!-- <img src="files/animated-gold-glitter-hearts.gif" alt="animated gold glitter hearts", height="28" width="63" class="floats">  -->
<h4>Acknowledgments</h4>

<small> (Have I missed something? Let me know!) </small>

<ul class="spaced">
<img src="files/animated-gold-glitter-hearts.gif" alt="animated gold glitter hearts", height="28" width="63" margin="20px">

<li>
<a href="https://arxiv.org/abs/2305.19148">Mitigating Label Biases for In-context Learning</a> &ndash; Yu Fei, Yifan Hou, Zeming Chen, Antoine Bosselut &ndash; 2023
</li>

<li>
<a href="https://arxiv.org/abs/2305.02582">On the Expressivity Role of LayerNorm in Transformers' Attention</a> &ndash; Shaked Brody, Uri Alon, Eran Yahav &ndash; 2023
</li>

<li>
<a href="https://arxiv.org/abs/2210.09404">Measures of Information Reflect Memorization Patterns</a> &ndash; Rachit Bansal, Danish Pruthi, Yonatan Belinkov &ndash; 2022
</li>

<li>
<a href="https://arxiv.org/abs/2202.12172">Overcoming a Theoretical Limitation of Self-Attention</a> &ndash; David Chiang and Peter Cholak &ndash; 2022
</li>


<li>
<a href="https://arxiv.org/abs/2105.14491">How attentive are Graph Attention Networks?</a> &ndash; Shaked Brody, Uri Alon, Eran Yahav &ndash; 2021
</li>


<li>
<a href="https://dl.acm.org/doi/abs/10.1145/3445814.3446732">Autonomous NIC offloads</a> &ndash; Boris Pismenny, Haggai Eran, Aviad Yehezkel, Liran Liss, Adam Morrison, and Dan Tsafrir &ndash; 2021
</li>


<li>
<a href="https://arxiv.org/abs/1906.03398">The regulator problem for the one-dimensional Schrodinger equation via the backstepping approach</a> &ndash; Hua-Cheng Zhou and George Weiss &ndash; 2019
</li>

<img src="files/animated-gold-glitter-hearts.gif" alt="animated gold glitter hearts", height="28" width="63">

</ul>

