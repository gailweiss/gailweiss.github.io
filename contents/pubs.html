<!-- SETTITLE Publications and Talks -->

<h4>Publications <span class='notbold'>(Separate Talks Below)</span></h4>

<i>(Notes: <b>1.</b> If you don&apos;t have access to github or arxiv, contact me for a copy. <b>2.</b> Google Colaboratory notebooks look messy, but press the <q>open with colab</q> button and they sort themselves out.)</i>

<br><br><br>

<ul>


<li>
    <b> Could ChatGPT get an engineering degree? Evaluating higher education vulnerability to AI assistants &ndash; PNAS 2024 </b> 
    <br>
    Beatriz Borges, Negar Foroutan, Deniz Bayazit, Anna Sotnikova, Syrielle Montariol, Tanya Nazaretzky, Mohammadreza Banaei, Alireza Sakhaeirad, Philippe Servant, Seyed Parsa Neshaei, Jibril Frej, Angelika Romanou, <i>Gail Weiss</i>, Sepideh Mamooler, Zeming Chen, Simin Fan, Silin Gao, Mete Ismayilzada, Debjit Paul, Philippe Schwaller, Sacha Friedli, Patrick Jermann, Tanja K&auml;ser, Antoine Bosselut, EPFL Grader Consortium, EPFL Data Consortium.
    <br> 
    <a href="https://arxiv.org/abs/2408.11841">PDF</a>
</li>

<br><br>


<li>
    <b> Improving Autoformalization using Type Checking &ndash; Preprint 2024 </b> 
    <br>
    Auguste Poiroux, <i>Gail Weiss</i>, Viktor Kunc&#780ak, Antoine Bosselut.
    <br> 
    <a href="https://arxiv.org/abs/2406.07222">PDF</a>
</li>

<br><br>



<li>
    <b>What Formal Languages can Transformers Express? A Survey &ndash; TACL 2024 </b> 
    <br>
    Lena Strobl, William Merrill, <i>Gail Weiss</i>, David Chiang, Dana Angluin.
    <br> 
    <a href="https://arxiv.org/abs/2311.00208">PDF</a>
</li>

<br><br>

 
<li>
    <b>Discovering Knowledge-Critical Subnetworks in Pretrained Language Models &ndash; EMNLP 2024 </b> 
    <br>
    Deniz Bayazit, Negar Foroutan, Zeming Chen, <i>Gail Weiss</i>, Antoine Bosselut.
    <br> 
    <a href="https://arxiv.org/abs/2310.03084">PDF</a>
</li>

<br><br>
 

<li>
    <b>RECKONING: Reasoning through Dynamic Knowledge Encoding &ndash; NeurIPS 2023 </b> 
    <br>
    Zeming Chen, <i>Gail Weiss</i>, Eric Mitchell, Asli Celikyilmaz, Antoine Bosselut.
    <br> 
    <a href="https://arxiv.org/abs/2305.06349">PDF</a> // <a href="https://github.com/eric11eca/reckoning-metakg">Github</a>
</li>

<br><br>


<li>
    <b>Neural Sequence Models: A Formal Lens &ndash; Technion 2023 (PhD Thesis) </b> 
    <br>
    <i>Gail Weiss</i>.
    <br> 
    <a href="files/thesis-gail-weiss.pdf">PDF</a> // 
    <a href="files/phd_seminar.pdf">Seminar slides</a> // 
    <a href="https://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-info.cgi/2023/PHD/PHD-2023-01">Faculty link</a><br>
</li>

<br><br>

<li>
    <b>Thinking Like Transformers &ndash; ICLR Blogposts 2023 </b> 
    <br>
    Sasha Rush, <i>Gail Weiss</i>.
    <br> 
    <a href="https://srush.github.io/raspy/">Blog post</a> //
    <a href="https://github.com/srush/raspy">RASPy - RASP-like embedded DSL</a>
</li>


<br><br>

<li>
    <b>Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples (Extended Version) &ndash; Springer Machine Learning 2022  </b>
    <br>
    <i>Gail Weiss</i>, Yoav Goldberg, Eran Yahav.
    <br> 
    <a = href="files/dfa-extraction-journal.pdf">Downloadable PDF (pre-Springer formatting)</a> //
    <a href="https://link.springer.com/article/10.1007/s10994-022-06163-2">Springer closed access link</a> //
    <a href="https://link.springer.com/epdf/10.1007/s10994-022-06163-2?sharing_token=ez9FeHiBADqx83NDn0smUPe4RwlQNchNByi7wbcMAY6tZ-C7vrgO2t7JiQulXBgtfBtaypFAQ9AJvCJjFEWqD3NTYShO_tYcW82ezRlgVnLz8fpN3Qmho1e5ZX7Lv9FUaKp58jAR8b7UOQB0EqaTPY1OltWWWzJTCe-y2XKSvds%3D">Springer PDF viewer (no downloads)</a> // 
    <a href="https://github.com/tech-srl/lstar_extraction">Github</a> 
    <br>
    Poster and talks linked for the regular version listed below (ICML 2018)
    <br>
    Published in the <a href="https://link.springer.com/journal/10994/topicalCollection/AC_9b98083c2072263208781c379a2810cc">Special Issue on Grammatical Inference</a> 
</li>

<br><br>


<li>
    <b>Thinking Like Transformers &ndash; ICML 2021 </b> 
    <br>
    <i>Gail Weiss</i>, Yoav Goldberg, Eran Yahav.
    <br> 
    <a href="https://arxiv.org/abs/2106.06981">PDF</a> // 
    <a href="https://github.com/tech-srl/RASP">Github (RASP REPL)</a> // 
    <a href="https://github.com/tech-srl/RASP-exps">Github (Experiments)</a> // 
    <a href="files/RASP_poster.png">Poster</a> 
    <br>
    5 min talk (ICML 2021): 
    <a href="files/RASP_5.pdf">Slides</a>, 
    <a href="https://slideslive.com/embed/presentation/38959389?url=https%3A%2F%2Ficml.cc%2Fvirtual%2F2021%2Fposter%2F9995&origin=https%3A%2F%2Ficml.cc">Talk</a> // 
    1 hour talk (Edinburgh 2021): 
    <a href="files/RASP_45.pdf">Slides</a>, 
    <a href="https://www.youtube.com/watch?v=H-x_upYg-JY">Talk</a>
</li>

<br><br>

<li>
    <b>Synthesizing Context-free Grammars from Recurrent Neural Networks &ndash; TACAS 2021 </b> 
    <br>
    Daniel Yellin, <i>Gail Weiss</i>. 
    <br>
    <a href="https://arxiv.org/abs/2101.08200">PDF</a> // 
    <a href="https://github.com/tech-srl/RNN_to_PRS_CFG">Github</a> // 
    20 min talk (TACAS 2021): 
    <a href="https://www.morressier.com/article/synthesizing-contextfree-grammars-recurrent-neural-networks/604907f41a80aac83ca25cf9">Slides and Talk</a>
</li>

<br><br>

<li>
    <b>A Formal Hierarchy of RNN Architectures &ndash; ACL 2020 </b>
    <br>
    William Merrill, <i>Gail Weiss</i>, Yoav Goldberg, Roy Schwartz, Noah A. Smith, Eran Yahav.  
    <br>
    <a href="https://arxiv.org/abs/2004.08500">PDF</a> // 
    <a href="https://lambdaviking.com/post/rr-hierarchy/">Will's Blog Post</a>// 
    10 min talk (ACL 2020): 
    <a href="https://slideslive.com/38928908/a-formal-hierarchy-of-rnn-architectures">Talk</a>
</li>

<br><br>

<li>
    <b>Learning Deterministic Weighted Automata with Queries and Counterexamples &ndash; NeurIPS 2019 </b> 
    <br>
    <i>Gail Weiss</i>, Yoav Goldberg, Eran Yahav.  
    <br>
    <a href="https://arxiv.org/abs/1910.13895">PDF</a> // 
    <a href="https://github.com/tech-srl/weighted_lstar">Github</a> // 
    <a href="files/neurips_2019_poster.pdf">Poster</a>
</li>

<br><br>

<li>
    <b>On the Practical Computational Power of Finite Precision RNNs for Language Recognition &ndash; ACL 2018 </b> 
    <br>
    <i>Gail Weiss</i>, Yoav Goldberg, Eran Yahav.
    <br> 
    <a href="https://arxiv.org/pdf/1805.04908.pdf">PDF</a> // 
    <a href="https://github.com/tech-srl/counting_dimensions">Github</a> // 
    10 min talk (ACL 2018): 
    <a href="https://vimeo.com/285806108">Talk</a> // 
    <a href="https://drive.google.com/file/d/1Ya4Dhih8JU-do9t4gcxkU4pgb9Gm8dmo/view?usp=sharing">Google Colaboratory</a>
    <br>
    <ul>Note: This paper is mostly an observation that LSTMs can implement a counting mechanism and GRUs can&apos;t, so the code here is just a demonstration of this.</ul>
</li>

<br><br>

<li>
    <b>Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples &ndash; ICML 2018 </b> 
    <br>
    <i>Gail Weiss</i>, Yoav Goldberg, Eran Yahav. 
    <br>
    <a href="https://arxiv.org/pdf/1711.09576.pdf">PDF</a> // 
    <a href="https://github.com/tech-srl/lstar_extraction">Github</a> // 
    <a href="files/icml_2018_poster.pdf">Poster</a> 
    (with thanks to Xiaokun Luan for corrections!) //
    10 min talk (ICML 2018): 
    <a href="https://youtu.be/ym64TaaHnT8?t=2766">Talk</a> // 
    <a href="https://drive.google.com/file/d/1tkJK1rJVEg9e-QcWOxErDb3cQq9UR-yR/view">Google Colaboratory</a> <br>
    Extended version listed above (Springer 2022)
</li>
</ul>

<br>

<h4>Talks <span class='notbold'>(Independent of Publications)</span></h4>

<ul>

<li>
<b>Formal Abstractions of Neural Sequence Models &ndash; ICGI 2021 </b> (Tutorial, 1.5hrs) 
<br>
<a href="files/ICGI_2021_slides.pdf">Slides</a>, 
<a href="https://www.youtube.com/watch?v=J89zTx5vs7A">Talk</a>
</li>

</ul>

